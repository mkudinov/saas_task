{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Proper names detector</h1>\n",
    "<p>Training binary classifier indentifying if the input string is a proper name</p>\n",
    "<h3>Approach</h3>\n",
    "<p>Basic idea is to train Random Forest classifier with in-word character n-grams as input features</p>\n",
    "<p><b>Features:</b> Character n-grams of decent length incorporate almost all reasonable features for proper names detection: gazetteer, affixes, prefixes, punctuation, usual character sequences. To differentiate suffixes and prefixes we add symbol '_' at the beginning and the end of each word.</p>\n",
    "<p><b>Classifier:</b> To avoid design of complex features (e.g. suffix='Abd'&amp;prefix='la') we use non-linear classifier. GradientBoosting is buggy in sklearn-0.18. Deep learning requires more manual programming and hyper-parameter optimizations. RandomForest is a good option though the resulting model may be quite large.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import randint\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import cross_validation\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data preparation</h3>\n",
    "<p><i>proper_names.txt</i> -- proper names from <i>persondata_en.nt</i> with stripped XML-tags and end-word markers added. We will use only half of them.</p>\n",
    "<p><i>geo_names.txt</i> -- geographic names from dbpedia <i>geonames_links_en.ttl</i> with stripped XML-tags and end-word markers added</p>\n",
    "<p><i>company_names.txt</i> -- companies listed on NYMEX (quite few, actually) with end-word markers added</p>\n",
    "<p><i>random_phrases.txt</i> -- 200k random 1-4 grams of most frequent words from English, Spanish, German, Italian and French with end-word markers added</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PROPER_NAMES_FILE_PATH = '/data/home/mkudinov/Data/proper_names/proper_names.txt'\n",
    "GEO_NAMES_FILE_PATH = '/data/home/mkudinov/Data/proper_names/geo_names.txt'\n",
    "COMPANY_NAMES_FILE_PATH = '/data/home/mkudinov/Data/proper_names/company_names.txt'\n",
    "RANDOM_PHRASES_FILE_PATH = '/data/home/mkudinov/Data/proper_names/random_phrases.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_dataset():\n",
    "    dataset = []\n",
    "    with open(PROPER_NAMES_FILE_PATH, 'r') as source_file:\n",
    "        for line in source_file:\n",
    "            rn = randint(1,100) #use half of the list of proper names\n",
    "            if rn <= 50:\n",
    "                dataset.append(unicode(line, 'utf-8').strip())\n",
    "    labels = [1] * len(dataset)\n",
    "    print \"Proper Names: %s\" % len(dataset) \n",
    "    lens = [0] * 3\n",
    "    for i, source in enumerate([GEO_NAMES_FILE_PATH, COMPANY_NAMES_FILE_PATH, RANDOM_PHRASES_FILE_PATH]):\n",
    "        with open(source, 'r') as source_file:\n",
    "            for line in source_file:\n",
    "                dataset.append(unicode(line, 'utf-8').strip())\n",
    "                lens[i] +=1\n",
    "    print \"Geo: %s Company: %s Random: %s\" % (lens[0], lens[1], lens[2])\n",
    "    labels += [0] * sum(lens)\n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fit model parameters and print metrics\n",
    "def model_fit(clf_, x_train_, y_train_, cv_): \n",
    "    clf_.fit(x_train_, y_train_)\n",
    "    train_pred = clf_.predict(x_train_)\n",
    "    train_prob = clf_.predict_proba(x_train_)[:,1]\n",
    "    print \"\\nModel Report\"\n",
    "    print \"Accuracy : %.4g\" % metrics.accuracy_score(y_train_, train_pred)\n",
    "    print \"Precision : %.4g\" % metrics.precision_score(y_train_, train_pred)\n",
    "    print \"Recall : %.4g\" % metrics.recall_score(y_train_, train_pred)\n",
    "    print \"F-1 measure : %.4g\" % metrics.f1_score(y_train_, train_pred)\n",
    "    print \"AUC Score (Train): %f\" % metrics.roc_auc_score(y_train_, train_prob)\n",
    "    if cv_ > 0:\n",
    "        cv_score = cross_validation.cross_val_score(clf_, x_train_, y_train_, cv=cv_, scoring='roc_auc', n_jobs=jobs)\n",
    "        print \"CV Score : Mean - %.7g | Std - %.7g | Min - %.7g | Max - %.7g\" % (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<h3>Read data and extract features</h3>\n",
    "<p>We use ngrams of 3<=n<=8 with more than 70 occurences in different phrases</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proper Names: 534311\n",
      "Geo: 326222 Company: 2690 Random: 200000\n",
      "Number of features:  93754\n"
     ]
    }
   ],
   "source": [
    "X, Y = read_dataset()\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.25)\n",
    "vectorizer = CountVectorizer(strip_accents='unicode', analyzer='char_wb', min_df=70, ngram_range=(3,8), binary=True)\n",
    "x_train_bin = vectorizer.fit_transform(x_train)\n",
    "x_test_bin = vectorizer.transform(x_test)\n",
    "print \"Number of features: \",len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Grid search</h3>\n",
    "Grid search with cross-validation on maximum depth of tree. Not surpisingly the more the better :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=False, n_jobs=6,\n",
       "       param_grid={'max_depth': [20, 30, 40, 50, 60, 70]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_depth = {'max_depth':range(20,80,10)}\n",
    "gsearch2 = GridSearchCV(estimator = RandomForestClassifier(n_estimators=100), \n",
    "param_grid = param_depth, scoring='roc_auc',n_jobs=6,iid=False, cv=5)\n",
    "gsearch2.fit(x_train_bin,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mean: 0.88721, std: 0.00152, params: {'max_depth': 20}, mean: 0.90948, std: 0.00115, params: {'max_depth': 30}, mean: 0.92608, std: 0.00201, params: {'max_depth': 40}, mean: 0.93723, std: 0.00096, params: {'max_depth': 50}, mean: 0.94501, std: 0.00096, params: {'max_depth': 60}, mean: 0.95236, std: 0.00027, params: {'max_depth': 70}] {'max_depth': 70} 0.952358981906\n"
     ]
    }
   ],
   "source": [
    "print gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_\n",
    "best_depth = gsearch2.best_params_['max_depth']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Test best performing classifier</h3>\n",
    "Best performing model has max_depth=70. Check test and train accuracy, AUC, P, R and F-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy : 0.889\n",
      "Precision : 0.9381\n",
      "Recall : 0.8341\n",
      "F-1 measure : 0.8831\n",
      "AUC Score (Train): 0.966983\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, max_depth=best_depth).fit(x_train_bin, y_train)\n",
    "model_fit(clf, x_train_bin, y_train, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_pred = clf.predict(x_test_bin)\n",
    "test_prob = clf.predict_proba(x_test_bin)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy : 0.8722\n",
      "Precision : 0.9192\n",
      "Recall : 0.8173\n",
      "F-1 measure : 0.8652\n",
      "AUC Score (Train): 0.954025\n"
     ]
    }
   ],
   "source": [
    "print \"\\nModel Report\"\n",
    "print \"Accuracy : %.4g\" % metrics.accuracy_score(y_test, test_pred)\n",
    "print \"Precision : %.4g\" % metrics.precision_score(y_test, test_pred)\n",
    "print \"Recall : %.4g\" % metrics.recall_score(y_test, test_pred)\n",
    "print \"F-1 measure : %.4g\" % metrics.f1_score(y_test, test_pred)\n",
    "print \"AUC Score (Train): %f\" % metrics.roc_auc_score(y_test, test_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>50 misclassified strings<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "errors = []\n",
    "for i in range(len(y_test)):\n",
    "    if test_pred[i] != y_test[i]:\n",
    "        sample = re.sub(r'_', '', x_test[i])\n",
    "        if y_test[i] == 1:\n",
    "            errors.append((sample, 'FN'))\n",
    "        else:\n",
    "            errors.append((sample, 'FP'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goldie goldthorpe              FN\n",
      "subhash misra                  FN\n",
      "micah kogo                     FN\n",
      "malaefou mackenzie             FN\n",
      "elena lunda                    FN\n",
      "nicoya                         FP\n",
      "rado krošelj                   FN\n",
      "chitra magimairaj              FN\n",
      "raoul trujillo                 FN\n",
      "kitso mokaila                  FN\n",
      "ashley green                   FP\n",
      "neal abberley                  FN\n",
      "virignin                       FP\n",
      "chiquito filipe do carmo       FN\n",
      "pilar seurat                   FN\n",
      "håkon col                      FP\n",
      "handover mars jellyfish        FP\n",
      "nancy cato                     FN\n",
      "dale neufeld                   FN\n",
      "greg halsey-brandt             FN\n",
      "regis pitbull                  FN\n",
      "felix cheong                   FN\n",
      "ramiro prialé                  FN\n",
      "yiorgo moutsiaras              FN\n",
      "silvana mangano                FN\n",
      "sia carol                      FP\n",
      "lola cornero                   FN\n",
      "clem ohameze                   FN\n",
      "jacques awoke                  FP\n",
      "cynthia sikes                  FN\n",
      "jainzhug                       FP\n",
      "saskatoon                      FP\n",
      "ekaterina savova-nenova        FN\n",
      "wongawilli                     FP\n",
      "pernille dupont                FN\n",
      "stylistics anders snuffle      FP\n",
      "chester adgate congdon         FN\n",
      "mankapur                       FP\n",
      "hayyim habshush                FN\n",
      "qamar abbas                    FN\n",
      "bree schaaf                    FN\n",
      "melissa ng                     FN\n",
      "miss fame                      FN\n",
      "pradeep tamta                  FN\n",
      "chatura herath                 FN\n",
      "feliciano vierra tavares       FN\n",
      "andorra                        FP\n",
      "prentice mulford               FN\n",
      "marktoffingen                  FP\n",
      "grover resinger                FN\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    print u'{:<30} {}'.format(errors[i][0], errors[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
